<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Scanner Emocional Real - Álibis Digital</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.18.0/dist/tf.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/face-landmarks-detection@1.0.0/dist/face-landmarks-detection.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/face-api.js@0.22.2/dist/face-api.min.js"></script>
    <style>
        .sensor-active { background-color: rgba(34, 211, 238, 0.1); border-color: rgba(34, 211, 238, 0.5); }
        .sensor-inactive { background-color: rgba(239, 68, 68, 0.1); border-color: rgba(239, 68, 68, 0.5); }
    </style>
</head>
<body class="bg-gray-900 text-white min-h-screen">
    <div class="container mx-auto px-4 py-8">
        <h1 class="text-3xl font-bold text-center mb-8">Scanner Emocional Real</h1>
        
        <div class="grid grid-cols-1 md:grid-cols-2 gap-6">
            <!-- Visualização da Câmera -->
            <div class="bg-gray-800 p-4 rounded-lg">
                <h2 class="text-xl font-semibold mb-4">Visualização</h2>
                <div class="relative">
                    <video id="video-input" width="100%" autoplay muted playsinline class="rounded-lg border border-cyan-500"></video>
                    <canvas id="video-canvas" class="hidden"></canvas>
                </div>
                
                <div class="grid grid-cols-3 gap-2 mt-4">
                    <div class="bg-gray-700 p-2 rounded text-center">
                        <div class="text-cyan-400">Expressão</div>
                        <div id="emotion-value" class="font-bold">--</div>
                    </div>
                    <div class="bg-gray-700 p-2 rounded text-center">
                        <div class="text-cyan-400">Confiança</div>
                        <div id="confidence-value" class="font-bold">--</div>
                    </div>
                    <div class="bg-gray-700 p-2 rounded text-center">
                        <div class="text-cyan-400">Status</div>
                        <div id="face-status" class="font-bold">Inativo</div>
                    </div>
                </div>
            </div>
            
            <!-- Controles -->
            <div class="bg-gray-800 p-4 rounded-lg">
                <h2 class="text-xl font-semibold mb-4">Controles</h2>
                
                <div class="space-y-4">
                    <button id="start-btn" class="w-full bg-cyan-600 hover:bg-cyan-700 text-white py-2 px-4 rounded transition">
                        Iniciar Análise
                    </button>
                    
                    <div class="bg-gray-700 p-3 rounded">
                        <h3 class="font-medium mb-2">Dados da Câmera</h3>
                        <div id="camera-data" class="text-sm space-y-1"></div>
                    </div>
                    
                    <div class="bg-gray-700 p-3 rounded">
                        <h3 class="font-medium mb-2">Últimas Detecções</h3>
                        <div id="detections-log" class="text-sm space-y-1"></div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        // Estado da aplicação
        const state = {
            isAnalyzing: false,
            stream: null,
            faceDetector: null,
            lastDetection: null,
            detectionCount: 0
        };

        // Elementos DOM
        const dom = {
            videoInput: document.getElementById('video-input'),
            videoCanvas: document.getElementById('video-canvas'),
            startBtn: document.getElementById('start-btn'),
            emotionValue: document.getElementById('emotion-value'),
            confidenceValue: document.getElementById('confidence-value'),
            faceStatus: document.getElementById('face-status'),
            cameraData: document.getElementById('camera-data'),
            detectionsLog: document.getElementById('detections-log')
        };

        // Inicialização
        document.addEventListener('DOMContentLoaded', async () => {
            await loadModels();
            setupEventListeners();
            
            // Verifica suporte a APIs necessárias
            checkSupport();
        });

        // Verifica suporte do navegador
        function checkSupport() {
            const supported = !!navigator.mediaDevices && 
                            !!window.AudioContext && 
                            !!window.TensorFlow;
            
            if (!supported) {
                alert('Seu navegador não suporta todas as funcionalidades necessárias. Use Chrome ou Edge mais recentes.');
            }
        }

        // Carrega modelos de ML
        async function loadModels() {
            try {
                // Carrega modelos do Face-API.js (detecção facial e emoções)
                await faceapi.nets.tinyFaceDetector.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models');
                await faceapi.nets.faceLandmark68Net.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models');
                await faceapi.nets.faceExpressionNet.loadFromUri('https://justadudewhohacks.github.io/face-api.js/models');
                
                console.log('Modelos de IA carregados com sucesso');
                logSystemMessage('Modelos de IA carregados com sucesso');
            } catch (error) {
                console.error('Erro ao carregar modelos:', error);
                logSystemMessage('Erro ao carregar modelos de IA: ' + error.message);
            }
        }

        // Configura listeners de eventos
        function setupEventListeners() {
            dom.startBtn.addEventListener('click', toggleAnalysis);
        }

        // Inicia/para a análise
        async function toggleAnalysis() {
            if (state.isAnalyzing) {
                await stopAnalysis();
            } else {
                await startAnalysis();
            }
        }

        // Inicia a análise
        async function startAnalysis() {
            try {
                // Solicita permissão para câmera
                state.stream = await navigator.mediaDevices.getUserMedia({
                    video: {
                        width: 640,
                        height: 480,
                        facingMode: 'user'
                    },
                    audio: false
                });
                
                // Configura o vídeo
                dom.videoInput.srcObject = state.stream;
                
                // Configura o canvas para processamento
                dom.videoCanvas.width = dom.videoInput.videoWidth;
                dom.videoCanvas.height = dom.videoInput.videoHeight;
                
                // Atualiza UI
                state.isAnalyzing = true;
                dom.startBtn.textContent = 'Parar Análise';
                dom.startBtn.classList.remove('bg-cyan-600');
                dom.startBtn.classList.add('bg-red-600');
                dom.faceStatus.textContent = 'Ativo';
                dom.faceStatus.classList.add('text-green-400');
                
                // Mostra informações da câmera
                const videoTrack = state.stream.getVideoTracks()[0];
                const settings = videoTrack.getSettings();
                dom.cameraData.innerHTML = `
                    <div>Resolução: ${settings.width}x${settings.height}</div>
                    <div>Taxa de quadros: ${settings.frameRate || 'N/A'} fps</div>
                    <div>Dispositivo: ${videoTrack.label || 'N/A'}</div>
                `;
                
                logSystemMessage('Análise iniciada com sucesso');
                
                // Inicia o loop de detecção
                detectFaces();
                
            } catch (error) {
                console.error('Erro ao iniciar análise:', error);
                logSystemMessage('Erro ao iniciar análise: ' + error.message);
                alert('Não foi possível acessar a câmera. Verifique as permissões.');
            }
        }

        // Para a análise
        async function stopAnalysis() {
            if (state.stream) {
                state.stream.getTracks().forEach(track => track.stop());
                state.stream = null;
            }
            
            // Atualiza UI
            state.isAnalyzing = false;
            dom.startBtn.textContent = 'Iniciar Análise';
            dom.startBtn.classList.remove('bg-red-600');
            dom.startBtn.classList.add('bg-cyan-600');
            dom.faceStatus.textContent = 'Inativo';
            dom.faceStatus.classList.remove('text-green-400');
            
            // Limpa o vídeo
            dom.videoInput.srcObject = null;
            
            logSystemMessage('Análise parada');
        }

        // Detecta faces e emoções
        async function detectFaces() {
            if (!state.isAnalyzing) return;
            
            try {
                // Captura o frame atual do vídeo
                const video = dom.videoInput;
                const canvas = dom.videoCanvas;
                const ctx = canvas.getContext('2d');
                
                ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
                
                // Detecta faces com Face-API.js
                const detections = await faceapi.detectAllFaces(
                    canvas, 
                    new faceapi.TinyFaceDetectorOptions()
                ).withFaceLandmarks().withFaceExpressions();
                
                if (detections.length > 0) {
                    // Pega a primeira face detectada
                    const detection = detections[0];
                    
                    // Determina a emoção predominante
                    const expressions = detection.expressions;
                    let dominantEmotion = 'neutral';
                    let maxConfidence = 0;
                    
                    for (const [emotion, confidence] of Object.entries(expressions)) {
                        if (confidence > maxConfidence) {
                            maxConfidence = confidence;
                            dominantEmotion = emotion;
                        }
                    }
                    
                    // Atualiza estado
                    state.lastDetection = {
                        emotion: dominantEmotion,
                        confidence: maxConfidence,
                        timestamp: new Date()
                    };
                    
                    state.detectionCount++;
                    
                    // Atualiza UI
                    updateDetectionUI(dominantEmotion, maxConfidence);
                    
                    // Registra no log
                    logDetection(dominantEmotion, maxConfidence);
                } else {
                    // Nenhuma face detectada
                    dom.emotionValue.textContent = '--';
                    dom.confidenceValue.textContent = '--';
                }
                
                // Continua o loop
                requestAnimationFrame(detectFaces);
                
            } catch (error) {
                console.error('Erro na detecção facial:', error);
                logSystemMessage('Erro na detecção: ' + error.message);
                setTimeout(detectFaces, 1000);
            }
        }

        // Atualiza a UI com os dados da detecção
        function updateDetectionUI(emotion, confidence) {
            // Mapeia emoções para cores
            const emotionColors = {
                happy: 'text-green-400',
                sad: 'text-blue-400',
                angry: 'text-red-400',
                fearful: 'text-purple-400',
                disgusted: 'text-yellow-400',
                surprised: 'text-orange-400',
                neutral: 'text-gray-400'
            };
            
            const colorClass = emotionColors[emotion] || 'text-gray-400';
            
            // Atualiza elementos DOM
            dom.emotionValue.textContent = translateEmotion(emotion);
            dom.emotionValue.className = `font-bold ${colorClass}`;
            dom.confidenceValue.textContent = `${Math.round(confidence * 100)}%`;
        }

        // Traduz emoções para português
        function translateEmotion(emotion) {
            const translations = {
                happy: 'Feliz',
                sad: 'Triste',
                angry: 'Raiva',
                fearful: 'Medo',
                disgusted: 'Nojo',
                surprised: 'Surpresa',
                neutral: 'Neutro'
            };
            
            return translations[emotion] || emotion;
        }

        // Registra detecções no log
        function logDetection(emotion, confidence) {
            const now = new Date();
            const timeStr = now.toLocaleTimeString();
            const emotionStr = translateEmotion(emotion);
            const confidenceStr = Math.round(confidence * 100);
            
            // Limita o log a 10 entradas
            const logEntries = dom.detectionsLog.innerHTML.split('</div>').filter(Boolean);
            if (logEntries.length >= 10) {
                logEntries.shift();
            }
            
            // Adiciona nova entrada
            const newEntry = `<div class="py-1 border-b border-gray-600">
                [${timeStr}] ${emotionStr} (${confidenceStr}%)
            </div>`;
            
            dom.detectionsLog.innerHTML = [...logEntries, newEntry].join('</div>') + '</div>';
        }

        // Registra mensagens do sistema
        function logSystemMessage(message) {
            console.log(message);
        }
    </script>
</body>
</html>
